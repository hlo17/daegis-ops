円卓 原則 v0.2（ドラフト）

1) コーディネーターは“持ち回り”
    •    毎タスク開始時に各AIが自己申告スコアを返す（例：speed, quality, creativity, confidence各0–10）。
    •    スコア×直近実績の加重平均でトップがその回のコーディネーターに就任。
    •    タイムアウト（例：3分）で未返答は0点扱い。膠着時は“メタ仲裁枠”（任意の別AI）に一回だけ委譲。

2) 自治型 投票・スコアリング
    •    フォーマットを厳格なJSONで統一（後述プロンプト参照）。
    •    コーディネーターは各AIの回答をマージして“最初の経路案”を提示（並列 or 逐次）。
    •    実行後、事後スコア（所要時間、再作業回数、ユーザー満足度）をLedgerへ自動記録。次回の重み付けに反映。

3) 競合調整（NotebookLMの文字数制限への対応）

NotebookLMを“中立レビュー役”に据えるのは有効ですが、文字数（~1600文字）制限が現実にボトルネック。そこで二段構えにします。
    •    段1：圧縮器（Compactor）→ ルーブリック要約
競合している各AI出力を、事前定義のスロット制テンプレに圧縮（各300–400字以内）。
スロット例：
    •    目的（1文）/ 手順（箇条書き）/ リスク（最大3個）/ 依存関係（最大3つ）/ 検証方法（1段落）
    •    段2：仲裁（Arbiter）
NotebookLM（またはGemini）に「圧縮済み要約のみ」を渡し、差分抽出＋合成提案を要求。
返ってきた“合成案”を、元のフル出力に参照リンク（ID）でひも付けして完全版を再構成。

これでLM側へ渡すトークンを意図的に小さく保てます。中立ロールは“完全情報”を扱わず要点のみで判定する設計に切り替えます。

---














円卓 システム


最高。では“そのまま貼って運用できる”v0.2を一気に出します（A/B/C すべて微修正反映済み）。
ポイントだけ先に：
    •    固定 enum を汎用 pattern に変更（将来AI追加OK）
    •    タスク側の目的重み（速度/品質/創造性）を導入 → 当回スコアに反映
    •    歴史データの正規化は分位点（p10–p90）で外れ値耐性UP
    •    仲裁は二値フラグをやめて priority_alpha ∈ [0,1]（0=品質、1=速度）一本化
    •    週次の外れ値は IQR→MAD、ドリフトは EWM z スコアも併用

⸻

A. 役割の持ち回り & 自治投票（v0.2）

{
  "$id": "urn:daegis:roundtable:v0.2",
  "self_declared_score_schema": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "properties": {
      "task_id": { "type": "string", "minLength": 6, "maxLength": 64 },
      "ai_name": {
        "type": "string",
        "pattern": "^[A-Za-z0-9._-]{2,32}$",
        "description": "モデルの識別名（将来追加に備え汎用）"
      },
      "model_version": { "type": "string" },
      "response_time_ms": { "type": "integer", "minimum": 0 },
      "cost_estimate": { "type": "number", "minimum": 0 },
      "scores": {
        "type": "object",
        "properties": {
          "speed":      { "type": "integer", "minimum": 0, "maximum": 10 },
          "quality":    { "type": "integer", "minimum": 0, "maximum": 10 },
          "creativity": { "type": "integer", "minimum": 0, "maximum": 10 },
          "confidence": { "type": "integer", "minimum": 0, "maximum": 10 }
        },
        "required": ["speed", "quality", "creativity", "confidence"],
        "additionalProperties": false
      },
      "why": { "type": "string", "maxLength": 100 },
      "needed_inputs": { "type": "array", "items": { "type": "string" } },
      "constraints":   { "type": "array", "items": { "type": "string" } }
    },
    "required": ["ai_name", "scores", "why"],
    "additionalProperties": false
  },

  "task_meta_schema": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "properties": {
      "task_id": { "type": "string" },
      "priority_weights": {
        "type": "object",
        "properties": {
          "speed":      { "type": "number", "minimum": 0, "maximum": 1 },
          "quality":    { "type": "number", "minimum": 0, "maximum": 1 },
          "creativity": { "type": "number", "minimum": 0, "maximum": 1 }
        },
        "required": ["speed", "quality", "creativity"],
        "additionalProperties": false
      },
      "alpha": { "type": "number", "minimum": 0, "maximum": 1, "default": 0.6 },
      "cooldown": {
        "type": "object",
        "properties": {
          "recent_window": { "type": "integer", "minimum": 1, "default": 3 },
          "max_wins":      { "type": "integer", "minimum": 1, "default": 2 },
          "factor":        { "type": "number", "minimum": 0.0, "maximum": 1.0, "default": 0.95 }
        },
        "required": ["recent_window", "max_wins", "factor"]
      }
    },
    "required": ["task_id", "priority_weights"]
  },

  "coordinator_selection_algorithm": {
    "description": "自己申告（current）× 実績（historical）× 目的重み（priority）× クールダウンで総合判定",
    "weighting_formula": "total = alpha * current + (1 - alpha) * historical * cd",
    "current": "current = Wspeed*speed + Wquality*quality + Wcreativity*creativity + 0.2*confidence（Wはpriority_weightsを正規化し和=1。confidenceは固定0.2寄与・上限1.0でクリップ）",
    "historical": "historical = 10 * [ 0.3*(1 - qnorm(lead_time)) + 0.3*(1 - qnorm(rework_count)) + 0.4*(user_rating/5) ]",
    "qnorm": "分位点正規化：qnorm(x) = clip((x - p10)/(p90 - p10), 0, 1)",
    "alpha_policy": "default 0.6 / 緊急0.8 / 研究0.4",
    "cooldown": "直近recent_windowでの当選数>max_winsならcd=factor（例0.95）、そうでなければ1.0",
    "timeouts": "AI応答30秒で打切り→スコア0。全体2分で打切り、利用可能AIのみで選出（最低2未満なら人間介入）",
    "no_response": "3連続未応答で実績計算から一時除外し警告",
    "tie_break": "user_rating高→直近3件合成高→task_hashシードの擬似乱数",
    "output": "最高totalのAIをCoordinatorに"
  }
}


⸻

B. 競合調整（圧縮→仲裁、v0.2）

{
  "$id": "urn:daegis:arbitration:v0.2",
  "stage1_compressor": {
    "description": "各AIが提案を定型で圧縮。長さチェックはサーバ側で集計。",
    "input_schema": {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "type": "object",
      "properties": {
        "task_description": { "type": "string", "maxLength": 500 },
        "ai_name": { "type": "string", "pattern": "^[A-Za-z0-9._-]{2,32}$" }
      },
      "required": ["task_description", "ai_name"],
      "additionalProperties": false
    },
    "output_schema": {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "type": "object",
      "properties": {
        "ai_name": { "type": "string" },
        "purpose": { "type": "string", "maxLength": 100 },
        "steps": { "type": "array", "items": { "type": "string", "maxLength": 50 }, "minItems": 1, "maxItems": 5 },
        "risks": { "type": "array", "items": { "type": "string", "maxLength": 50 }, "maxItems": 3 },
        "dependencies": { "type": "array", "items": { "type": "string", "maxLength": 50 }, "maxItems": 3 },
        "verification": { "type": "string", "maxLength": 150 }
      },
      "required": ["ai_name", "purpose", "steps", "risks", "dependencies", "verification"],
      "additionalProperties": false
    }
  },

  "stage2_arbitrator": {
    "description": "中立AIが圧縮JSONを比較・合成。速度/品質の重みは priority_alpha ∈ [0,1]（0=品質、1=速度）",
    "input_schema": {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "type": "object",
      "definitions": {
        "compressed_proposal": { "$ref": "urn:daegis:arbitration:v0.2#/stage1_compressor/output_schema" }
      },
      "properties": {
        "compressed_proposals": { "type": "array", "items": { "$ref": "#/definitions/compressed_proposal" }, "minItems": 2, "maxItems": 5 },
        "task_description": { "type": "string", "maxLength": 500 },
        "priority_alpha": { "type": "number", "minimum": 0, "maximum": 1, "default": 0.5 },
        "score_snapshots": {
          "type": "object",
          "description": "各AIのcurrentスコア（Aの自己申告）",
          "additionalProperties": {
            "type": "object",
            "properties": {
              "speed": { "type": "number" },
              "quality": { "type": "number" },
              "creativity": { "type": "number" },
              "confidence": { "type": "number" }
            },
            "required": ["speed", "quality", "creativity", "confidence"]
          }
        }
      },
      "required": ["compressed_proposals", "task_description"]
    },

    "output_schema": {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "type": "object",
      "properties": {
        "differences_table": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "aspect": { "type": "string", "maxLength": 50 },
              "proposals": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "ai_name": { "type": "string" },
                    "summary": { "type": "string", "maxLength": 100 }
                  },
                  "required": ["ai_name", "summary"]
                }
              }
            },
            "required": ["aspect", "proposals"]
          }
        },
        "adopted_items": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "ai_name": { "type": "string" },
              "item": { "type": "string", "maxLength": 100 },
              "reason_code": { "type": "string", "enum": ["evidence","coherence","risk_lower","speed_gain","novelty"] },
              "reason_note": { "type": "string", "maxLength": 80 }
            },
            "required": ["ai_name", "item", "reason_code"]
          }
        },
        "rejected_items": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "ai_name": { "type": "string" },
              "item": { "type": "string", "maxLength": 100 },
              "reason_code": { "type": "string", "enum": ["weak_evidence","low_coherence","high_risk","too_slow","redundant"] },
              "reason_note": { "type": "string", "maxLength": 80 }
            },
            "required": ["ai_name", "item", "reason_code"]
          }
        },
        "synthesized_proposal": { "type": "string", "maxLength": 500 },
        "evaluation_weights": {
          "type": "object",
          "properties": {
            "speed_weight":   { "type": "number", "minimum": 0, "maximum": 1 },
            "quality_weight": { "type": "number", "minimum": 0, "maximum": 1 },
            "other_weight":   { "type": "number", "minimum": 0, "maximum": 1 },
            "formula_used":   { "type": "string" }
          },
          "required": ["speed_weight", "quality_weight", "other_weight", "formula_used"]
        }
      },
      "required": ["differences_table", "adopted_items", "rejected_items", "synthesized_proposal", "evaluation_weights"]
    },

    "scoring_formula": "eval = (priority_alpha * speed + (1 - priority_alpha) * quality) + 0.2 * (creativity + confidence)/2"
  }
}


⸻

C. 継続進化サイクル（週次ふりかえり→重み更新、v0.2）

{
  "$id": "urn:daegis:evolution:v0.2",
  "selected_kpis": {
    "lead_time":   { "description": "平均完了時間（分）", "collection_method": "Slack/Ledgerの開始・終了から抽出" },
    "rework_count":{ "description": "再作業回数の平均", "collection_method": "仲裁ログのreworkイベント集計" },
    "user_rating": { "description": "1-5の満足度", "collection_method": "Slack投票/手入力→週次平均" },
    "conflict_rounds": { "description": "競合解決ラウンド数", "collection_method": "仲裁出力から抽出" },
    "handoff_count":   { "description": "ハンドオフ回数", "collection_method": "リレー履歴の相関IDで集計" }
  },

  "handling_missing_outliers": {
    "missing_values": "直近有効値の線形補間。週欠損が多い場合は全体平均×0.8で補完しフラグ付与。",
    "outliers": "MAD法で検知しメジアン補完（IQRより小標本に強い）。"
  },

  "recent_bias_control": {
    "method": "EMA",
    "formula": "ema_t = a * composite_t + (1 - a) * ema_{t-1}",
    "alpha": 0.3
  },

  "weight_update_formula": {
    "composite_score": "Σ (norm_k * w_k)。悪化指標は (1 - norm)。w = {'lead_time':0.2,'rework_count':0.2,'user_rating':0.3,'conflict_rounds':0.15,'handoff_count':0.15}",
    "role_prob_update": "delta = ema_t - ema_{t-1}; new_prob_i = old_prob_i + delta * 0.1 * (i+1)/n → その後で正規化",
    "handoff_note": "handoffは多い=悪と限らない。しきい値τ以下は無影響、超過分のみペナルティに（例：penalty=max(0,(handoff-τ)/τ)）"
  },

  "drift_detection": {
    "rules": [
      "|ema_t - ema_{t-1}| > 0.1",
      "z = |ema_t - ema_{t-1}| / EWMstd > 2.0"
    ],
    "auto_reallocation": "ドリフト検知時、role_prob_updateを即時適用し、Coordinatorへ通知"
  }
}


⸻

使い方の超シンプル運用手順
    1.    投票収集：各AIに A の self_declared_score_schema で返答させる＋タスク側 task_meta_schema を用意。
    2.    選出：式どおり total を算出、クールダウン反映、Coordinator決定。
    3.    競合時：Bのステージ1で圧縮 → ステージ2に priority_alpha と score_snapshots を渡して仲裁。
    4.    記録：CのKPIをLedgerに記録、週次にEMA/ドリフトで重みを更新。

このまま Map に載せれば “円卓の自治” を即スタートできます。必要なら、YAMLの既定値ファイルや、Slack ワークフロー／Zap の入力・出力マッピングも作りますよ。

